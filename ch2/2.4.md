# 高斯消去法的误差分析 #

两步法获得 $$ Ax=b $$ 解的误差界：

1) **向后误差分析**

求解线性代数方程组

$$ 
Ax = b 
$$

设计一个算法 $$ alg(A, b) $$, 求出一个数值解

$$
\hat x = alg(A, b)
$$ 

数值解 $$ \hat x $$，相当于求解下面扰动后的方程：

$$ 
(A + \delta A)\hat x = b + \delta b 
$$

其中 $$ \delta A $$ 和 $$ \delta b $$ 称为算法的 **向后误差**。

而 $$ \delta x = \hat{x} - x $$ 的界是我们希望界定的 **计算误差**。

2）利用扰动分析理论分析, 如前面得到的 ( 2.2 ) 和 ( 2.5 )。

$$
\frac{ \| \delta x \|}{ \| \hat x \|} \leq \| A^{-1} \|\cdot \| A \|\cdot 
\left(\frac{ \| \delta A \|}{ \| A \|} 
+ \frac{ \| \delta b \| }{ \| A \| \| \hat x \| } \right)\quad ( 2.3 ).
$$

* (2.3) 算出的误差界往往太大。

$$
\| \delta x \| = \| A^{-1}r \|\leq \| A^{-1} \|\cdot \| A\hat{x} - b \|.\quad ( 2.5 )
$$

* ( 2.5 ) 更易计算, 在实践中更常使用。

本节的两个目标：

* 展示如何实施高斯消去法，使得向后误差 $$ \delta A $$ 和 $$ \delta b $$ 尽量小：

$$
\begin{align}
   \frac{ \| \delta A \| }{ \| A \| } = O(\varepsilon), \quad \frac{ \| \delta b \| }{ \| b \| } = O(\varepsilon).
\end{align}
$$

* 获得同时**易于计算(cheapness)**和**紧致(tightness)**的误差界。
    + 紧致就是说算出的误差界尽量接近真实的误差。
    + 易于计算和紧致的误差界可能不存在。

## 为什么要选主元？ ##

给定一个矩阵, 这里用 4 位的十进制的浮点数表示， 

$$
A = \begin{bmatrix}
    0.0001 & 1 \\
    1 & 1 \\
\end{bmatrix}
$$

易知 $$ \| A \|_\infty \| A^{-1} \|_\infty \approx 4$$。 


$$
L = \begin{bmatrix}
    1 & 0 \\
    fl(\frac{1}{10^{-4}}) & 1
\end{bmatrix}
$$
其中 $$ fl(\frac{1}{10^{-4}}) = 10^4 $$ 

$$
U = \begin{bmatrix}
    10^{-4} & 1 \\
    & fl(1 - 10^4*1) 
\end{bmatrix}
$$
其中 $$ fl(1 - 10^4*1) = -10^4 $$ ( 大数吃小数 )


$$
LU = \begin{bmatrix}
    1 & 0 \\
    10^4 & 1 \\
\end{bmatrix}
\begin{bmatrix}
    10^{-4} & 1 \\  & -10^4 
\end{bmatrix}
= \begin{bmatrix}
    10^{-4} & 1 \\ 1 & 0
\end{bmatrix}
\not=\begin{bmatrix}
    0.0001 & 1 \\
    1 & 1 \\
\end{bmatrix}
$$

* $$ a_{22} $$ 换成其它小的数,  $$ LU $$ 分解的结果是一样的， 因为 $$ fl(a_{22} - 10^4*1) = -10^4 $$。
* 这种现象称为 **数值不稳定 ( numerical instability )**。
* 取 $$ b = [1, 2]^T $$， 用 $$ LU $$ 分解求解 $$ Ax=b $$。  
* $$ \| A-LU \| $$  太大。 
* $$ \kappa(A) \approx 4 $$， 而 $$ \kappa(LU) \approx 10^8 $$ 
* 列主元可以解决数值不稳定的问题。


$$
L = \begin{bmatrix}
    1 & 0 \\ fl( .0001 / 1) & 1
\end{bmatrix}
\begin{bmatrix}
    1 & 0 \\ 0.0001 & 1
\end{bmatrix}
$$


$$
U = \begin{bmatrix}
    1 & 1 \\ 0 & fl(1 - .0001\cdot 1) 
\end{bmatrix}
\begin{bmatrix}
    1 & 1 \\ 0 & 1
\end{bmatrix}
.
$$





**注意** 如果 $$ LU $$ 分解过程中出现很大的量，则 $$ A $$ 中的元素减去这些量时，
就会出现大数吃小数的现象。

## 正式的高斯消去法误差分析 ##

因为浮点数运算中舍入误差的存在， 用 $$ LU $$ 分解算法来计算 $$ Ax=b  $$ 会带来向
后误差 $$ \delta A $$ ，这个向后误差有两部分组成

1. $$ LU $$ 分解算法带来的误差， 即 $$ A = LU + E $$。
1. 求解 $$ LU x = b $$ 带来的向后误差， 即 $$ ( L+\delta L ) ( U + \delta U )
   \hat{x} = b$$。 

本小节就来分析这两种误差的界。

首先，分析 $$ A $$ 的元素 $$ a_{jk} $$ 与 $$ L\cdot U $$ 的元素 $$ \sum_{i=1}^{k} l_{ji}u_{ik} $$ 的差别。 

$$
u_{j k}=a_{j k}-\sum_{i=2}^{j-1} l_{j i} u_{i k} \text{ with } j < k.
$$


$$
l_{j k}=\frac{a_{j k}-\sum_{i=1}^{k-1} l_{j i} u_{i k}}{u_{k k}} \text{ with } j
\leq k
$$

两个向量做内积运算，浮点运算结果满足如下的关系式：

$$
\mathrm{f}\left(\sum_{i=1}^{d} x_{i} y_{i}\right)=\sum_{i=1}^{d} x_{i} y_{i}\left(1+\delta_{i}\right) \quad \text { with }\left|\delta_{i}\right| \leq d \varepsilon
$$


$$
u_{j k}=\left(a_{j k}-\sum_{i=1}^{j-1} l_{j i} u_{i k}\left(1+\delta_{i}\right)\right)\left(1+\delta^{\prime}\right)
$$


$$
\begin{aligned} a_{j k}=& \frac{1}{1+\delta^{\prime}} u_{j k} \cdot l_{j j}+\sum_{i=1}^{j-1} l_{j i} u_{i k}\left(1+\delta_{i}\right) \quad \text { since } l_{j j}=1 \\=& \sum_{i=1}^{j} l_{j i} u_{i k}+\sum_{i=1}^{j} l_{j i} u_{i k} \delta_{i} \\ & \text { with }\left|\delta_{i}\right| \leq(j-1) \varepsilon \text { and } 1+\delta_{j} \equiv \frac{1}{1+\delta^{\prime}} \\ & \equiv \sum_{i=1}^{j} l_{j i} u_{i k}+E_{j k} \end{aligned}
$$


$$
\left|E_{j k}\right|=\left|\sum_{i=1}^{j} l_{j i} \cdot u_{i k} \cdot \delta_{i}\right| \leq \sum_{i=1}^{j}\left|l_{j i}\right| \cdot\left|u_{i k}\right| \cdot n \varepsilon=n \varepsilon(|L| \cdot|U|)_{j k}
$$


$$
l_{j k}=\left(1+\delta^{\prime \prime}\right)\left(\frac{\left(1+\delta^{\prime}\right)\left(a_{j k}-\sum_{i=1}^{k-1} l_{j i} u_{i k}\left(1+\delta_{i}\right)\right)}{u_{k k}}\right)
$$


$$
\begin{aligned} a_{j k} &=\frac{1}{\left(1+\delta^{\prime}\right)\left(1+\delta^{\prime \prime}\right)} u_{k k} l_{j k}+\sum_{i=1}^{k-1} l_{j i} u_{i k}\left(1+\delta_{i}\right) \\ &=\sum_{i=1}^{k} l_{j i} u_{i k}+\sum_{i=1}^{k} l_{j i} u_{i k} \delta_{i} & \text { with } 1+\delta_{k} \equiv \frac{1}{\left(1+\delta^{\prime}\right)\left(1+\delta^{\prime \prime}\right)} \\ & \equiv \sum_{i=1}^{k} l_{j i} u_{i k}+E_{j k} \end{aligned}
$$



## 课后作业 ##


1. 第一章 Question 1.10



